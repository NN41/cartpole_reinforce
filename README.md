# Implement REINFORCE for CartPole-v1
This project implements the REINFORCE algorithm from scratch to solve the OpenAI Gym CartPole-v1 environment using Pytorch.

## Project Goals
- Implement REINFORCE from scratch with an MLP policy, focusing on improving average return.
- Investigate the impact of a value function baseline.
- Investigate the impact of different loss functions: the sample average over trajectories only, the sample average over all state-action pairs, using full return, using reward-to-go, reward-to-go with value baseline.
- Experiment with network architecture.
- Follow Chapter 11 of Goodfellow while implementing the algorithm
- Extend to VPG

## REINFORCE
The training progress is very sensitive to the learning rate (using SGD). Because of this it's important to make sure that, once tuned, the magnitude of the gradients don't change. That's why we need to take the scale the double sum by the total number of log probabilities, not only by the number of trajectories. Otherwise, as the training progresses, the trajectories collect higher rewards and the magnitude of the gradients change. What was a suitable learning rate in the first epoch, will be too large by the Nth epoch. To see this, consider a single episode per epoch, in which case our loss function is proportional to $\Sigma_{t=0}^T \log \pi_\theta(a_t|s_t) R(\tau)$. It is clear that the magnitude of the gradient grows as trajectories become longer.

Looking at the most recent expression of policy gradient being a double sum of grad-log-probs scaled by the correspoding episode's return, we see how REINFORCE learns. REINFORCE tries to maximize the double sum of grad-log-probs as a proxy for the expected return. It does so by increasing the log-probabilities, or equivalantly, the probabilities of choosing the action $a_t$ under state $s_t$. This doesn't depend on how well action $a_t$ actually was, the only thing matters is the entire trajectory's return. It can be that action $a_0|s_0$ was actually really bad, but if the agent managed to recover and the rest of the episode goes great, then REINFORCE still makes $a_0|s_0$ more likely. Note that other (better) actions are actually made less likely this way, which makes the algorithm very sensitive to the randomly chosen actions at each time step. In the case of the cartpole, if the policy always selects the wrong action with high likelihood on the first timestep, but always manages to recover, then this wrong first action will be reinforced over time by pure overrepresentation in the training data of on-policy trajectories. This motivates discounting the future rewards.

Note that the reinforcement of actions happens proportionally to the associated reward, since the contribution of the grad-log-prob of a certain state-action pair in the policy gradient is proportional to this associated reward. In the naive policy gradient, that is the entire trajectory's return. If one trajectory performs extremely well relative to the other trajectories by pure luck, then all actions in that trajectory will be reinforced by an disproportional amount as well, even the actions that were irrelevant or even detrimental to the success of that trajectory. 

According to wikipedia: the score function (i.e. grad-log-prob(a_t|s_t)) can be interpreted as the direction in parameter space we need to move to increase the probability of taking action a_t in state s_t. The (naive) policy gradient is then the weighted avrage of all possible directions that increase the probability of taking any of the actions in any of the corresponding states, but weighted by the associated episode's return. As per the VPG page of Spinning Up, policy gradients learn the optimal policy by pushing up probabilities of actions that lead to higher return, while pushing down probabilities that lead to lower return.

The desired consequence of using reward-to-go and baselines is that it produces lower-variance estimates of the policy gradient, that is, we estimate by the gradient policy by a sample mean of weight * Grad(log_prob). Reducing the variance of this estimate comes down to reducing the variance of "weight", since Grad(log_prob) depends on the policy network architecture in remains unaffected by different choices of "weight" (such as "return" or "reward-to-go" or "reward-to-go - baseline" etc). Thus, to save compute, as a proxy, we will focus on measuring the variance of "weight" instead of weight * Grad(log_prob), the latter of which would be way more compute intensive. 

The core fundamentals of policy gradient methods, is that we want to associate some measure of goodness to an action that we take, then improve the likelihood of good actions based on that measure. Depending on which goodness measure we choose, we make a trade-off between bias and variance: using returns gives unbiased estimates while having high variance. Using a value function approximation significantly lowers variance at the cost of introducing bias (depending on how bad the apprxoimation is).

### Baselines
Spinning Up Part 3 suggests using MSE for learning our MLP that is approximating the on-policy value function. As discussed by Goodfellow, you can derive MSE loss through MLE by assuming that targets are produced through the underlying process plus some Gaussian noise. In this context, that would mean assuming that a trajectory's return is Gaussian distributed around the value function, which generally seems like an unrealistic assumption to make, especially in the cartpole context, since we would expect a lot of short trajectories and few long ones, leading to a heavily skewed returns distribution.

Rather, the choice for MSE loss is one of simplicity and pragmatism. It's a well-understood loss function and minizing MSE loss learns to predict the mean of the target (trajectory returns) given the input (state). Here, the mean of the target is exactly the on-policy value function.  

## Experiments & Results
- Adam performs way better than SGD
- One of the simplest algorithms seems to be a 1-hidden layer MLP with 4 hidden units and ReLU activation for the policy network, then Adam with lr = 0.01 and 50 episodes per epoch, 100 epochs, using reward-to-go and average over all. This achieves around avg 500 (maximum) return after 100 epochs.  

## Obstacles
- It took a lot of effort getting the REINFORCE algorithm with a value function MLP baseline working. In particular, it seems that I was way overtraining the value network during after each round of 1000 episodes. I also used the parameters from the previous training round and I didnt' reinitilize the optimizer in the beginning. The results is that the network likely got overfit to a first batch of trajectories using policy pi_k, then wasn't able to adapt anymore to the trajectories from policy pi_{k+1}. When training the network only a single epoch and reinitilizatin both the network and Adam optimizer for every new policy pi_k, we are making progress again. I'm surprised to see that the test MSE loss of the value network doesn't decrease by more than 1%. Way more than such an improvement seems to suggest overfitting. During some epochs, the value network doesn't seem to learn anything at all, strangely. The question is that with such a badly fit value network, do you even get a noticeable decrease in variance?
- It seems that the biggest problem with the value network is overfitting to a certain distribution coming from policy pi_k, then being unable to adjust its parameters to fit the distribution shift for policy pi_{k+1}
- I also had to include an ReLU activation function at the output layer, since the value network was outputting negative values, which doesn't make sense for CartPole. Then I figured out this is disastrous for training the neural network. At each new policy, sometimes the network would be initialized with weights such that all inputs would lead to negative outputs, hence they would be squashed to zero for all training data and the gradients would be zero as well.
- The BIGGEST obstacle of this project was figuring out for how long to train the value network, how to set it up, etc. I really should use an experimentation framework to grid search the correct learning rate. Just use one epoch, reset every time, then slowly adjust the learning rate from 0 to 0.1 using SGD and see where you end up. The GAE paper mentions that using a value function introduces a bias. Maybe that is the problem, that if you train it too much, the bias overwhelms the benefits?
- What I struggle to understand: Using rewards-to-go with a baseline gives an unbiased estimate of the policy gradient, certainly. So even a badly-trained value network should not introduce bias, however it may introduce variance. But then why do I struggle so much with getting a baseline working? Maybe I should try to run with smaller learning rates, more epochs and more episodes per epoch.
- POTENTIALLY the fact that the value network at the terminated state still outputs some random number instead of 0 might have enough of an impact on the baseline values that it messes with the measure of goodness of actions towards the end of the episodes
- Some discussions with Gemini about using eval() when generating sample trajectories for the policy to be trained on. Gemini was suggesting a two-pass approach, where you use eval() when generating trajectories and then put it to train() and then use the action and states from the generated trajectories to get the log probs from the model in train mode. But this is wrong since the policy gradient as an expectation is assuming that the trajectories are sample according to the policy that is being updated (i.e. the policy network in train mode). So, you must have train enabled when generating actions and log probs that will be used for updating the network.
- It's important to use AI but be very careful. For example, in light of reproducibility, we decided to use the same seed for each first run of each experiment, each second run, each third run etc, by setting seed = base_seed + run_idx. However, Gemini 2.5 Pro Preview 06-05 put this seed setting in the wrong place, which meant that the seed did not get reset properly (even though the runs were using the correct seed, the seed was not reset). 
- Acknowledge that it is not pretty to be doing forward passes through the value function network even when it is not being used for the agent.